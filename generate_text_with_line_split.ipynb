{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, Activation\n",
    "from keras.layers import LSTM, TimeDistributed, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "fd = open('data/sentences.txt', 'r')\n",
    "for line in fd:\n",
    "    sentences.append(line[:-1])\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 29878\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 1277948\n"
     ]
    }
   ],
   "source": [
    "corpus = ' '.join(sentences)\n",
    "print('Corpus length:', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 72\n",
      "[' ', '!', '\"', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(corpus)))\n",
    "vocab_size = len(chars)\n",
    "print('Total chars:', len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i,c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_idx = [char_indices[c] for c in corpus]\n",
    "sentences_idx = []\n",
    "for sentence in sentences:\n",
    "    sentences_idx.append([char_indices[c] for c in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "dataset = sequence.pad_sequences(sentences_idx, maxlen=maxlen+1, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for sentence in dataset:\n",
    "    x.append(sentence[:-1])\n",
    "    y.append(sentence[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (29878, 100) y: (29878, 100)\n"
     ]
    }
   ],
   "source": [
    "x = np.concatenate([[np.array(i)] for i in x])\n",
    "y = np.concatenate([[np.array(i)] for i in y])\n",
    "print('x:', x.shape, 'y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 24, input_length=maxlen))\n",
    "model.add(LSTM(\n",
    "            128,\n",
    "            return_sequences=True,\n",
    "            dropout_U=0.2,\n",
    "            dropout_W=0.2,\n",
    "            consume_less='gpu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "            128,\n",
    "            return_sequences=True,\n",
    "            dropout_U=0.2,\n",
    "            dropout_W=0.2,\n",
    "            consume_less='gpu'))\n",
    "model.add(TimeDistributed(Dense(vocab_size)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_example(start, n_char, maxlen):\n",
    "    for i in range(n_char):\n",
    "        start_idx = [char_indices[c] for c in start]\n",
    "        z = sequence.pad_sequences([start_idx], maxlen=maxlen, truncating='pre')\n",
    "        pred = model.predict_classes(z, verbose=0)[0][-1]\n",
    "        start = start + indices_char[pred]\n",
    "    print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.fit(\n",
    "        x,\n",
    "        np.expand_dims(y, -1),\n",
    "        batch_size=64,\n",
    "        nb_epoch=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "29878/29878 [==============================] - 58s - loss: 1.1016    \n",
      "I am toud tous tous tous tous tous tous tous tous tous toun toun toun toun toun toun toun toun toun toun \n",
      "Epoch 1/1\n",
      "29878/29878 [==============================] - 58s - loss: 1.0263    \n",
      "I am to to to to tore to tore to tore to to to to the to the the the the the the the the the the the the \n",
      "Epoch 1/1\n",
      "29878/29878 [==============================] - 59s - loss: 0.9699    \n",
      "I am the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "Epoch 1/1\n",
      "29878/29878 [==============================] - 61s - loss: 0.9291    \n",
      "I am the wang.r..n.....................s the wang the wang the wang the wang the wang the wang the wang t\n",
      "Epoch 1/1\n",
      "29878/29878 [==============================] - 62s - loss: 0.8981    \n",
      "I am the san..................................t.r..t.r..t.r..t.r..t the be the sant the sant the sant the\n",
      "Epoch 1/1\n",
      "29878/29878 [==============================] - 62s - loss: 0.8699    \n",
      "I am the san.......t.r..t.r..t.r..t.r..t.r..t.r..t.r..t.r..t.r..t the be the be the be the be the be the \n",
      "Epoch 1/1\n",
      "29878/29878 [==============================] - 63s - loss: 0.8466    \n",
      "I am the wan..........t.r..t.r..t.r..t.r..t.r..t.r..t.r..t.r..t.r..t.r..t.r..t.r.r.r the be the be the wa\n",
      "Epoch 1/1\n",
      "29878/29878 [==============================] - 63s - loss: 0.8268    \n",
      "I am the Nor the Nor the Nor the Nor the Nor the Nor the Nor the Nor the Nor the Nor the Nor the Not the \n",
      "Epoch 1/1\n",
      "29878/29878 [==============================] - 63s - loss: 0.8104    \n",
      "I am the No................................t.r............n.r.r.r.r.r.r.r.r.re the want the be the be the\n",
      "Epoch 1/1\n",
      "29878/29878 [==============================] - 63s - loss: 0.7966    \n",
      "I am the No.................................................n.r.rs.rs and the be the be the be the be the\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train()\n",
    "    print_example('I am ', 100, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
